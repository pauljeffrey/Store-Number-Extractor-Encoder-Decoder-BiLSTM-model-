{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"xbO8OFyjaiKz","executionInfo":{"status":"ok","timestamp":1654596186057,"user_tz":-60,"elapsed":3553,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import os\n","from keras.layers import Dropout\n","import pandas as pd\n","\n","from tensorflow.keras.layers import Bidirectional\n","from tensorflow.keras.layers import Layer\n","from keras import layers\n","from tensorflow.keras.callbacks import LearningRateScheduler,EarlyStopping\n","\n","from time import time\n","from tensorflow.keras.metrics import Accuracy \n","#import tensorflow.keras.utils.pad_sequences as pad"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"q4MtwNrLevs-","executionInfo":{"status":"ok","timestamp":1654596186061,"user_tz":-60,"elapsed":12,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"}}},"outputs":[],"source":["\n","file_path  = '/content/drive/MyDrive/Deep Learning/Store Number Extractor'\n","os.chdir(file_path)"]},{"cell_type":"markdown","metadata":{"id":"VKbWAaI0Avfx"},"source":["## Define Model\n","We create an encoder-decoder with attention architecture and we model the data character wise as opposed to word-wise. The encoder is made up of one to several layers ( set as argument) or bidirectional GRU. This encoder goes through each character encoding it in the process. Now, the deeper the layers, the more information about the neighbouring characters is encoded in the index character. We use a default of 3 layers here to ensure that it encodes information from at least 2 to 4 neighbouring characters. However, this can be modified as an argument provided.\n","\n","The decoder part is made up of the attention, a GRU and a fully connected layer that outputs the predictions. The predictions are basically predicted probabilities of all characters in the dictionary. The dictionary contains all characters found in the dataset.\n","\n","### How It Works\n","First the addresses are preprocessed to a list of characters then, using an char-index look up table, these list of characters are converted to a list of indices. This is passed to an embedding layer (it is shared by both the encoder and decoder) that computes a n-dimensional representation of each of these characteres. This is then passed to the encoder (bidirectional GRU). The encoder processes it and produces an output which can be thought of as a high level encoded representation of the whole address (list of characters).\n","\n","The output of the encoder is then passed to the decoder. The decoder takes as input its previous state (hidden state) from its GRU (which is initialized to zero at the beginning of each training loop or inference), the previous character it predicted (predicted by the fully connected layer) and the the encoded address.\n","\n","Given its previous state, the decoder pays attention to the encoded address using its attention module to produce a context which should contain encoded information about what the next character should be. The decoder's GRU then takes this context and the previous character (after passing through the embedding layer) it predicted, concatenates  them to produce a current hidden state. This state is received as input by the decoder's fully connected layer to produce the predicted probabilities of all characters in its vocabulary (dictionary).\n","\n","We use a sparse categorical loss entropy as loss function and an adam optimizer as optimizer. Model and training function was created from the ground up. Models are well commented as well as other codes.\n","\n","We start by defining a maskedloss class, encoder, decoder and the full model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EevFv0sNbHQm"},"outputs":[],"source":["# Define the loss class\n","class MaskedLoss(tf.keras.losses.Loss):\n","  def __init__(self):\n","    self.name = 'masked_loss'\n","\n","    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n","        from_logits=True, reduction='none')\n","\n","  def __call__(self, y_true, y_pred):\n","    # Calculate the loss for each item in the batch.\n","    loss = self.loss(y_true, y_pred)\n","\n","    # Mask off the losses on the elements that were padded in the targets.\n","    mask = tf.cast(y_true != 0, tf.float32)\n","    loss *= mask\n","\n","    # Return the mean loss.\n","    return tf.reduce_mean(loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fz5MYOKBb3Up"},"outputs":[],"source":["# Build attention module implemented from original paper.\n","\n","class BahdanauAttention(tf.keras.Model):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","   \n","\n","  def call(self, enc_hidden, dec_hidden):\n","    # hidden here means 'hidden_size', enc_hidden is the encoded_address as explained above\n","    # enc_hidden shape == (batch_size, char_length, hidden)\n","    # dec_hidden shape == (batch_size,  hidden)\n","    \n","    # attention_hidden_layer shape == (batch_size, char_length, units)\n","    attention_hidden_layer = (tf.nn.tanh(self.W1(enc_hidden) +\n","                                         self.W2(dec_hidden)))\n","\n","    # score shape == (batch_size, char_length, 1)\n","    # Score is a function of the hidden of the decoder and the encoder hidden\n","    # This gives you an unnormalized score for each word.\n","    score = self.V(attention_hidden_layer)\n","\n","    # attention_weights shape == (batch_size, char_length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden)\n","    context_vector = attention_weights * enc_hidden # (batch_size, char_length, hidden)\n","    context_vector = tf.reduce_sum(context_vector, axis=1) #(batch_size, hidden)\n","\n","    return context_vector, attention_weights\n","\n","\n","# Build encoder model\n","class Encoder(tf.keras.Model):\n","  def __init__(self, units,encoder_layers= 3, dropout=0.05):\n","    # units : no of weights , encoder_layers : no of bidirectional GRU layers, dropout: percentage of weights to drop out (change to zero) during training.\n","    super(Encoder, self).__init__()\n","    # Define bidirectional GRU encoder\n","    self.enc_layers = []\n","    for i in range(encoder_layers):\n","      self.enc_layers.append(layers.Bidirectional(layers.GRU(units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform',\n","                                   dropout= dropout)))\n","\n","    # Define dropout \n","    self.dropout = tf.keras.layers.Dropout(dropout,trainable=True,)\n","    #self.embed_dropout  = tf.keras.layers.Dropout(dropout)\n","\n","  def call(self, x):    \n","    # first, we encode the full list of characters in the addresses:\n","    for enc_layer in self.enc_layers:\n","      x, state,_ = enc_layer(x) \n","\n","    # Apply dropout\n","    x = self.dropout(x)\n","    return x #, state\n","\n","\n","# Build decoder model\n","class Decoder(tf.keras.Model):\n","  def __init__(self, units , att_units, vocab_size, dropout =0.05):\n","    # units: no of weights, att_units: no of weights for the attention module, vocab_size: size of the unique characters in the dictionary\n","    # Initialize with parent class\n","    super(Decoder, self).__init__()\n","    self.units = units\n","\n","    # Decoder GRU\n","    self.gru = tf.keras.layers.GRU(units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    # self.gru2 = tf.keras.layers.GRU(units,\n","    #                                return_sequences=True,\n","    #                                return_state=True,\n","    #                                recurrent_initializer='glorot_uniform',\n","    #                                 dropout = dropout)\n","    \n","    #Define attention class for encoder:\n","    self.attention = BahdanauAttention(att_units)\n","  \n","    # Define dropout for GRU\n","    self.dropout = tf.keras.layers.Dropout(dropout,trainable=True)\n","\n","    # Define fully connected layer for prediction of character probabilities.\n","    self.fc = tf.keras.layers.Dense(vocab_size )\n","\n","  # Method resets the hidden state of the decoder model at the beginning of iteration\n","  def reset_state(self, batch_size): \n","    return tf.zeros((batch_size,self.units)) \n","    \n","  def call(self, prev_char, hidden, encoded_address ):\n","    # This decoder model predicts the store numbers (character by character) in 2 steps:\n","    # First, it uses the hidden state (hidden_state) to attend to the encoded address to get useful 'contextual' information.\n","    # second, it uses the embedded previous character and the contextual info to predict the next character of the target\n","\n","    # Expand the hidden state by 1 more dimension (batch_size, 1, hidden)\n","    hidden = tf.expand_dims(hidden,1)\n","\n","    # Use attention mechanism to get core information as context:\n","    context, _  = self.attention(encoded_address, hidden)\n","\n","    # concatenate prev_char embedding and context_vector then, pass to gru\n","    # Expand context by one more dimension first\n","    context = tf.expand_dims(context, 1)\n","    hidden, state = self.gru(tf.concat([prev_char, context], -1))\n","\n","\n","    # Apply dropout to the context:\n","    hidden = self.dropout(hidden)\n","\n","    # Run through a fully connected layer to predict softmax of next char:\n","    # output shape = (batch_size, vocab_size)\n","    output = self.fc(hidden)\n","\n","    # return output and hidden state of gru\n","    return output , state\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWsFFd0JkpCH"},"outputs":[],"source":["# Define full model\n","class NE_Extractor(tf.keras.Model):\n","  def __init__(self,units,embedding_dim, att_units, enc_vocab_size, dec_vocab_size= None, enc_layers=3, reg_emb = True, dropout=0.05):\n","    # initialize with parent class:\n","    super(NE_Extractor, self).__init__()\n","\n","    # initialize encoder:\n","    self.encoder  = Encoder(units,enc_layers, dropout)\n","\n","    # Initialize decoder:\n","    if dec_vocab_size != None:\n","      self.decoder  = Decoder(units, att_units, dec_vocab_size, dropout =dropout)\n","    else:\n","      self.decoder  = Decoder(units, att_units, enc_vocab_size, dropout =dropout)\n","\n","\n","    # Embedding layer\n","    self.embed = tf.keras.layers.Embedding(enc_vocab_size, embedding_dim,mask_zero=True)\n","    \n","    # Embed dropout\n","    if reg_emb: #if we want to regularize the embedding layer using dropout, create a unique dropout layer for it.\n","      self.embed_dropout = tf.keras.layers.Dropout(dropout, trainable=True)\n","      self.reg_emb = reg_emb # Set regularize embedding to True\n","    else: # else set regularize embedding to False\n","      self.reg_emb = False\n","    # Create accuracy metric\n","    self.accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n","\n","  # Define the customized training function:\n","  @tf.function\n","  def train_step(self, data):\n","    # Unpack the data.\n","    address, target = data\n","\n","    # Define batch_size, max_target_length for training loop:\n","    batch_size = tf.shape(target)[0]\n","    max_target_length = tf.shape(target)[1]\n","   \n","    # Set loss to 0.0\n","    loss = tf.constant(0.0)\n","\n","    # Reset accuracy state\n","    self.accuracy.reset_state()\n","\n","    # Set the first input to the decoder model to the '<p>' token which is the first element of the target string:\n","    dec_input = target[:,0]\n","    # Reset decoder state for every batch of training data\n","    dec_hidden = self.decoder.reset_state(batch_size)\n","\n","    with tf.GradientTape() as tape:   \n","      # Embedded each character of the address:\n","      address = self.embed(address)\n","\n","      if self.reg_emb:\n","        address = self.embed_dropout(address)\n","\n","      # Pass the embedded address as inputs to the encoder.\n","      # we use the returned hidden state of the encoder to initialize the decoder's hidden state:\n","      address = self.encoder(address)\n","\n","      # Use for loop to iterate through each char as we predict the next char:\n","      for i in range(1,max_target_length):\n","\n","        # Get embedding of previous word , shape = (batch_size, 1, embedding_dim)\n","        dec_input = self.embed(dec_input)\n","        dec_input= tf.expand_dims(dec_input, 1)\n","\n","        #print('Prev_char ', tf.shape(prev_char))\n","        # Add dropout to embed layer\n","        if self.reg_emb:\n","          dec_input = self.embed_dropout(dec_input)\n","\n","      # Make predictions\n","        # Pass dec_input (serves as previous char), dec_hidden and the encoded_address to the decoder.\n","        predictions , dec_hidden = self.decoder(dec_input, dec_hidden, address)\n","\n","        #print(tf.shape(predictions))#, tf.shape(dec_input))\n","        # Calculate the loss\n","        loss  = loss + self.loss(target[:,i],predictions)\n","\n","        accuracy = self.accuracy.update_state(target[:,i] , predictions)\n","        \n","        # Using teacher forcing:\n","        # Teacher forcing continually feeds the next correct char in the real string (target) to the model\n","        # instead of passing what the model predicted back into the model.\n","        dec_input = target[:, i]\n","\n","      \n","      # Average the loss over all non padding tokens.\n","      average_loss = loss  / tf.cast(max_target_length, tf.float32)\n","      # Get accuracy:\n","      acc = self.accuracy.result()\n","\n","      # Average the accuracy over al non_padding_tokens.\n","      #average_accuracy = accuracy / tf.cast(max_target_length, tf.float32)\n","\n","    # Apply an optimization step\n","    variables = self.trainable_variables \n","    gradients = tape.gradient(average_loss, variables)\n","    self.optimizer.apply_gradients(zip(gradients, variables))\n","\n","    # Return a dict mapping metric names to current value\n","    return {'batch_avg_loss': average_loss, 'accuracy': acc*100}\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"1cZHVcZiWUko"},"source":["# Data Exploration"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"1qZ1Z5s2p9ZW","executionInfo":{"status":"ok","timestamp":1654596191995,"user_tz":-60,"elapsed":745,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"}}},"outputs":[],"source":["data = pd.read_csv('extra_test.csv')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":990},"executionInfo":{"elapsed":389,"status":"ok","timestamp":1654596217176,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"U-KzRLCTvaip","outputId":"c4d66257-bbf7-43a4-ddff-af6f9fdfe9fe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["    Unnamed: 0                             transaction_descriptor  \\\n","0            0                     unitel 51635 communication inc   \n","1            1             michael angelo rosario 46996 solutions   \n","2            2                   raw materials services 30119 inc   \n","3            3                momentum capital partners 51916 llc   \n","4            4                                     eccocane 91888   \n","5            5                 antioch baptist church 86984 north   \n","6            6                     walker downey  associates 4216   \n","7            7                                canson 22592 talens   \n","8            8          missouri slope lutheran 26702 care center   \n","9            9                                 34131 w2 group inc   \n","10          10                          aztec plumbing 46696 corp   \n","11          11                       celerity broadband llc 56116   \n","12          12               alexian brothers 29052 lansdowne vlg   \n","13          13                     michelob ultra golf tour 38879   \n","14          14                   lafontaine  associates 98258 inc   \n","15          15                 95694 southern crescent pediatrics   \n","16          16                      local 520 ua 89863 federal cu   \n","17          17  electrical controls of houston inc dba 60108 e...   \n","18          18                  crossroads steel supply 10110 llc   \n","19          19                               nanette 13486 lepore   \n","20          20                         12094 new tradition realty   \n","21          21                            henderson eye ctr 79217   \n","22          22                     gk brand 8212 inc tribe global   \n","23          23             98008 st ann catholic school  bartlett   \n","24          24                  old paramus 54031 reformed church   \n","25          25                         global fox 69818 financial   \n","26          26                           new 40584 horizons house   \n","27          27                          enplus advisors inc 14827   \n","28          28                                    2565 auman mack   \n","29          29                                  tyler 17304 place   \n","\n","    store_number  \n","0          51635  \n","1          46996  \n","2          30119  \n","3          51916  \n","4          91888  \n","5          86984  \n","6           4216  \n","7          22592  \n","8          26702  \n","9          34131  \n","10         46696  \n","11         56116  \n","12         29052  \n","13         38879  \n","14         98258  \n","15         95694  \n","16         89863  \n","17         60108  \n","18         10110  \n","19         13486  \n","20         12094  \n","21         79217  \n","22          8212  \n","23         98008  \n","24         54031  \n","25         69818  \n","26         40584  \n","27         14827  \n","28          2565  \n","29         17304  "],"text/html":["\n","  <div id=\"df-f4f281bd-2c37-4912-98e4-6a8bbed401f9\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>transaction_descriptor</th>\n","      <th>store_number</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>unitel 51635 communication inc</td>\n","      <td>51635</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>michael angelo rosario 46996 solutions</td>\n","      <td>46996</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>raw materials services 30119 inc</td>\n","      <td>30119</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>momentum capital partners 51916 llc</td>\n","      <td>51916</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>eccocane 91888</td>\n","      <td>91888</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>antioch baptist church 86984 north</td>\n","      <td>86984</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>walker downey  associates 4216</td>\n","      <td>4216</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>canson 22592 talens</td>\n","      <td>22592</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>missouri slope lutheran 26702 care center</td>\n","      <td>26702</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>34131 w2 group inc</td>\n","      <td>34131</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>aztec plumbing 46696 corp</td>\n","      <td>46696</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>celerity broadband llc 56116</td>\n","      <td>56116</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>12</td>\n","      <td>alexian brothers 29052 lansdowne vlg</td>\n","      <td>29052</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>13</td>\n","      <td>michelob ultra golf tour 38879</td>\n","      <td>38879</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>14</td>\n","      <td>lafontaine  associates 98258 inc</td>\n","      <td>98258</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>15</td>\n","      <td>95694 southern crescent pediatrics</td>\n","      <td>95694</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>16</td>\n","      <td>local 520 ua 89863 federal cu</td>\n","      <td>89863</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>17</td>\n","      <td>electrical controls of houston inc dba 60108 e...</td>\n","      <td>60108</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>18</td>\n","      <td>crossroads steel supply 10110 llc</td>\n","      <td>10110</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>19</td>\n","      <td>nanette 13486 lepore</td>\n","      <td>13486</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>20</td>\n","      <td>12094 new tradition realty</td>\n","      <td>12094</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>21</td>\n","      <td>henderson eye ctr 79217</td>\n","      <td>79217</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>22</td>\n","      <td>gk brand 8212 inc tribe global</td>\n","      <td>8212</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>23</td>\n","      <td>98008 st ann catholic school  bartlett</td>\n","      <td>98008</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>24</td>\n","      <td>old paramus 54031 reformed church</td>\n","      <td>54031</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>25</td>\n","      <td>global fox 69818 financial</td>\n","      <td>69818</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>26</td>\n","      <td>new 40584 horizons house</td>\n","      <td>40584</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>27</td>\n","      <td>enplus advisors inc 14827</td>\n","      <td>14827</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>28</td>\n","      <td>2565 auman mack</td>\n","      <td>2565</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>29</td>\n","      <td>tyler 17304 place</td>\n","      <td>17304</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f4f281bd-2c37-4912-98e4-6a8bbed401f9')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f4f281bd-2c37-4912-98e4-6a8bbed401f9 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f4f281bd-2c37-4912-98e4-6a8bbed401f9');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":6}],"source":["data.head(30)"]},{"cell_type":"markdown","metadata":{"id":"IipVLRgLWaYI"},"source":["There are 30000 samples in the data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1654107992290,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"nmvNCt6CuYZr","outputId":"b495844e-2b10-44a9-c090-5cfc90d4761d"},"outputs":[{"data":{"text/plain":["30000"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["len(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDFwQtkVqtyi"},"outputs":[],"source":["_, address_column, target_column = data.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1654107992291,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"b06YsrFNUqz3","outputId":"b5ab630f-bb16-4b8b-f1d2-dfe914676dbc"},"outputs":[{"name":"stdout","output_type":"stream","text":["79\n"]},{"data":{"text/plain":["{' ',\n"," '0',\n"," '1',\n"," '2',\n"," '3',\n"," '4',\n"," '5',\n"," '6',\n"," '7',\n"," '8',\n"," '9',\n"," 'a',\n"," 'b',\n"," 'c',\n"," 'd',\n"," 'e',\n"," 'f',\n"," 'g',\n"," 'h',\n"," 'i',\n"," 'j',\n"," 'k',\n"," 'l',\n"," 'm',\n"," 'n',\n"," 'o',\n"," 'p',\n"," 'q',\n"," 'r',\n"," 's',\n"," 't',\n"," 'u',\n"," 'v',\n"," 'w',\n"," 'x',\n"," 'y',\n"," 'z',\n"," 'à',\n"," 'á',\n"," 'â',\n"," 'ã',\n"," 'å',\n"," 'ç',\n"," 'é',\n"," 'ê',\n"," 'ó',\n"," 'õ',\n"," 'ö',\n"," 'ù',\n"," 'ú',\n"," 'ü',\n"," 'ē',\n"," 'ě',\n"," 'ı',\n"," 'ō',\n"," 'ş',\n"," 'а',\n"," 'в',\n"," 'д',\n"," 'е',\n"," 'и',\n"," 'к',\n"," 'л',\n"," 'я',\n"," 'і',\n"," 'أ',\n"," 'ا',\n"," 'ب',\n"," 'ة',\n"," 'ت',\n"," 'د',\n"," 'س',\n"," 'ك',\n"," 'ل',\n"," 'م',\n"," 'ن',\n"," '化',\n"," '论',\n"," '进'}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["corpus = ''\n","for addr in data[address_column]:\n","  corpus += addr\n","\n","unique_characters = set(corpus)\n","print(len(unique_characters))\n","unique_characters"]},{"cell_type":"markdown","metadata":{"id":"ofC_NFhefyXd"},"source":["We see that there are 70 unique characters in the whole dataset. A lot of them aren't ascii characters. Additionally, most addresses will have capital letters, '-',  ',',  '()' etc in them. \n","\n","For the sake of simplicity, we represent all words outside the ascii characters with '[UNK]'. Here, I assume that there should be just english addresses in the dataset. Furthermore, we can safely do this because, I assume that characters shouldnt really have really strong relationships with the store numbers (at the character level). We also add start and end tags to the beginning and end of each character. \n","\n","We convert ' ' to <sp> and then we pad all addresses to the same length so they can be passed to the model at once.\n","\n","Below, we define some functions to do all this."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwNJKVQMuW33"},"outputs":[],"source":["# This function gets the address sample with the longest length\n","def get_max_length(column):\n","  max_len = 0\n","  for each_sample in column:\n","    if len(each_sample) > max_len:\n","      max_len = len(each_sample)\n","  return max_len\n","\n","# This function pads the samples to an equal length or rectangular array or matrix\n","def pad(column):\n","  # Get max_length\n","  max_len = get_max_length(column)\n","  for each_sample in column:\n","    if len(each_sample) < max_len: # if length of current sample is less than max_length, extend sample with the <p> pad tag.\n","      each_sample.extend(['<p>']*( max_len - len(each_sample)))\n","\n","  return \n","\n","# This function adds the <start>, <end> and <sp> tag to the data samples\n","def process_column(column):\n","  data = []\n","  for each_sample in column:\n","    sample_list = list(str(each_sample))\n","    sample_list.insert(0, '<start>')\n","    sample_list.append('<end>')\n","    for ind, char in enumerate(sample_list):\n","      if char == ' ':\n","        sample_list[ind] = '<sp>'\n","    data.append(sample_list)\n","  return data #, index\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"I_QQ-WoXHaig"},"outputs":[],"source":["# process addresses and targets\n","addresses = process_column(data[address_column])\n","targets = process_column(data[target_column])\n","#tf.keras.utils.pad_sequences(addresses, value= '<p>', padding='post')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1654108040954,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"n7HKD3mLpvZQ","outputId":"29f2f4ba-645c-46ce-e39a-3b977a28cc5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["unitel 51635 communication inc\n","['<start>', 'u', 'n', 'i', 't', 'e', 'l', '<sp>', '5', '1', '6', '3', '5', '<sp>', 'c', 'o', 'm', 'm', 'u', 'n', 'i', 'c', 'a', 't', 'i', 'o', 'n', '<sp>', 'i', 'n', 'c', '<end>']\n"]}],"source":["# Check out a sample\n","print(data[address_column][0])\n","print(addresses[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"mtpdRqz9oTMu"},"outputs":[],"source":["# pad addressess\n","pad(addresses)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YbvUJcvIv-Wg"},"outputs":[],"source":["# pad targets\n","pad(targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":545,"status":"ok","timestamp":1654108087864,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"0yHLXmrvwAwX","outputId":"ae7085b1-e93a-4215-f7e6-d9ca31629ca8"},"outputs":[{"name":"stdout","output_type":"stream","text":["['<start>', '9', '5', '6', '9', '4', '<end>'] ['<start>', 'u', 'n', 'i', 't', 'e', 'l', '<sp>', '5', '1', '6', '3', '5', '<sp>', 'c', 'o', 'm', 'm', 'u', 'n', 'i', 'c', 'a', 't', 'i', 'o', 'n', '<sp>', 'i', 'n', 'c', '<end>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>']\n"]}],"source":["print(targets[15], addresses[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":502,"status":"ok","timestamp":1654108110790,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"FiqmiJdYqnPT","outputId":"ddd99277-3264-40f9-a7fa-a96eb3a3270b"},"outputs":[{"data":{"text/plain":["103"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["len(addresses[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1654108111195,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"og0dGJJ2v7FW","outputId":"4d2faf18-1145-4760-e7e9-44270aaeb31b"},"outputs":[{"data":{"text/plain":["103"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["len( addresses[9871])"]},{"cell_type":"markdown","metadata":{"id":"h3GW5k6ShtYY"},"source":["We can see from above that all samples of dataset are of the same length. So, we create the vocabulary dictionary that holds all characters needed in the dataset. Here, we manually do this by selecting all ascii characters including numbers, artificially created tags and some other characters as shown below.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":623,"status":"ok","timestamp":1654108848950,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"PRZHV_cl4GJk","outputId":"9be9b43b-a2fb-46c2-e684-fe1b9ed082a0"},"outputs":[{"name":"stdout","output_type":"stream","text":["['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-', '#', '.', ',', '<sp>', '<start>', '<end>'] 43\n"]}],"source":["import string\n","vocab = [char for char in string.ascii_lowercase]\n","vocab.extend(['0', '1', '2', '3','4','5', '6', '7', '8', '9','-','#','.',',','<sp>','<start>','<end>'])\n","print(vocab, len(vocab))\n","\n","#dec_vocab = ['0', '1', '2', '3','4','5', '6', '7', '8', '9','<start>', '<end>','#','<sp>']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1654108849349,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"FCt74VrgxsQF","outputId":"5063bb7c-a2e7-47ba-f0d5-19cdd0aac1dc"},"outputs":[{"data":{"text/plain":["43"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["len(vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1654108849351,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"vCmsKWGrwCQn","outputId":"9bbe046e-5190-4ac7-f637-381d77019294"},"outputs":[{"name":"stdout","output_type":"stream","text":["['<start>', 'm', 'i', 'c', 'h', 'a', 'e', 'l', '<sp>', 'a', 'n', 'g', 'e', 'l', 'o', '<sp>', 'r', 'o', 's', 'a', 'r', 'i', 'o', '<sp>', '4', '6', '9', '9', '6', '<sp>', 's', 'o', 'l', 'u', 't', 'i', 'o', 'n', 's', '<end>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>', '<p>']\n","['<start>', '5', '1', '6', '3', '5', '<end>']\n"]}],"source":["print(addresses[1])\n","print(targets[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1654108849352,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"hL4EQm35wHLT","outputId":"01e630ce-a14f-4502-8295-7e0594242c5b"},"outputs":[{"data":{"text/plain":["103"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["get_max_length(addresses)"]},{"cell_type":"markdown","metadata":{"id":"kEFQkAJVi-oD"},"source":["We convert the list of list of characters (list of processed addresses) to an n-dimensional array below. \n","\n","Then we split the data set to train and test samples (25000: 5000 respectively)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Hbs1zwEg0sq2"},"outputs":[],"source":["addresses = np.array(addresses)\n","targets = np.array(targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1654108850322,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"Pg5EJTJZ2HD2","outputId":"1eb23c65-ba39-4e5e-af67-f358acd3932f"},"outputs":[{"data":{"text/plain":["(30000, 7)"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["targets.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vAMmjYPI0DUP"},"outputs":[],"source":["from sklearn.utils import shuffle\n","\n","addresses, targets= shuffle(addresses, targets, random_state=27)\n","n_samples= 25000\n","\n","train_addresses , train_targets = addresses[:n_samples], targets[:n_samples]\n","test_addresses, test_targets =  addresses[n_samples:] , targets[n_samples:]"]},{"cell_type":"markdown","metadata":{"id":"AzDxJSZ-jd46"},"source":["Now, we define our char-index lookup class from the keras library. We create 2: one to get characters from indexes and another to get indexes from characters. We also add the pad token because it will have to be masked by this class and mapped to 0.\n","\n","\n","This class will be used to convert our list of characters to a list of indexes or numbers downstream."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1654108851993,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"jreXFfM73HV0","outputId":"4062ffd0-5b90-4674-d48d-75605e1acf7e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[UNK] :-  1\n"," 1 :-  b'[UNK]'\n"]}],"source":["# object converts characters to indexes.\n","ids_from_chars = tf.keras.layers.StringLookup(\n","    vocabulary= vocab, mask_token='<p>')\n","\n","# object converts unique integers to their respective characters.\n","chars_from_ids = tf.keras.layers.StringLookup(\n","    vocabulary=vocab, invert=True, mask_token='<p>')\n","\n","# #For decoder vocabulary:\n","# dec_ids_from_chars = tf.keras.layers.StringLookup(\n","#     vocabulary= dec_vocab, mask_token='<p>')\n","\n","# # object converts unique integers to their respective characters.\n","# dec_chars_from_ids = tf.keras.layers.StringLookup(\n","#     vocabulary=dec_vocab, invert=True, mask_token='<p>')\n","\n","enc_vocab_size = len(ids_from_chars.get_vocabulary())\n","#dec_vocab_size = len(dec_chars_from_ids.get_vocabulary())\n","\n","\n","print('[UNK] :- ',ids_from_chars(['[UNK]']).numpy()[0])\n","print(' 1 :- ', chars_from_ids([1]).numpy()[0])\n","\n","# print('For decoders')\n","# print('1 :- ',dec_ids_from_chars(['1']).numpy()[0])\n","# print(' 3 :- ', dec_chars_from_ids([3]).numpy()[0])\n","\n","# print('<p> :- ',dec_ids_from_chars(['<p>']).numpy()[0])\n","# print(' 0 :- ', dec_chars_from_ids([0]).numpy()[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1654108853497,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"fClt-blm-iK1","outputId":"47636917-0034-44ce-b9b3-51a6498121f0"},"outputs":[{"data":{"text/plain":["45"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["enc_vocab_size"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1654108854009,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"kqnl6oi7-gPj","outputId":"6ca473f8-a84d-4b10-bf48-c5473c96c959"},"outputs":[{"name":"stdout","output_type":"stream","text":["['<p>', '[UNK]', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-', '#', '.', ',', '<sp>', '<start>', '<end>']\n"]},{"data":{"text/plain":["45"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["print(chars_from_ids.get_vocabulary())\n","len(chars_from_ids.get_vocabulary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jayqwtsq9q4L"},"outputs":[],"source":["# We convert both addresses and their targets to indexes\n","train_addresses = ids_from_chars(train_addresses)\n","test_addresses = ids_from_chars(test_addresses)\n","# train_targets = dec_ids_from_chars(train_targets)\n","# test_targets = dec_ids_from_chars(test_targets)\n","train_targets = ids_from_chars(train_targets)\n","test_targets = ids_from_chars(test_targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348,"status":"ok","timestamp":1654108862142,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"qHAd2JJy3Wmq","outputId":"4950e917-d872-4bdc-94c4-79048acb21c1"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(7,), dtype=int64, numpy=array([43, 37, 37, 33, 36, 35, 44])>"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["train_targets[0]"]},{"cell_type":"markdown","metadata":{"id":"3NQF6ixZknxV"},"source":["Now, we predefine a list of parameters that will be used to train the model. We create a dataset class from tensorflow library that processes our dataset in batches feeding it to the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sz_m0LuMy82p"},"outputs":[],"source":["TRAIN_BUFFER_SIZE = len(train_addresses)\n","BATCH_SIZE = 512\n","#TEST_BUFFER_SIZE = len(target)\n","\n","# This Dataset class shuffles the data and precreates batches for it.\n","dataset = tf.data.Dataset.from_tensor_slices((train_addresses, train_targets)).shuffle(TRAIN_BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE)\n","dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yzolvD4_1d3I"},"outputs":[],"source":["# enc_vocab_size = len(vocab)\n","# dec_vocab_size = len(dec_vocab)\n","embedding_dim = 512 #embedding layer size\n","hidden = 512 # GRU hidden units\n","att_units = hidden\n","epochs = 70\n","\n","\n","\n","# Initialize the learning rate for the model.\n","initial_learning_rate = 0.001\n","# We schedule a learning rate for every epoch using the formula below:\n","# For every epoch, learning rate is reduced using this formula.\n","decay = initial_learning_rate / epochs\n","\n","# We use a time based decay to schedule the learning rate which will be appropriate for online learning.\n","def lr_time_based_decay(epoch, lr):\n","    return lr * 1 / (1 + decay * epoch)"]},{"cell_type":"markdown","metadata":{"id":"VoPexP-NlHRA"},"source":["Now, we define, compile our custom model. Then, train the model on the dataset using the Dataset class we created earlier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wy-SAWgPxn4D"},"outputs":[],"source":["extractor= NE_Extractor(hidden,embedding_dim, att_units, enc_vocab_size, dec_vocab_size = None, reg_emb = False, dropout=0.2)\n","\n","# Configure the loss and optimizer\n","extractor.compile(\n","    optimizer=tf.optimizers.Adam(),\n","    loss=MaskedLoss(),\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzxcgLsmyXbq"},"outputs":[],"source":["checkpoint_path = os.path.join(os.path.abspath(os.curdir),'ckpt/train')\n","dec_checkpoint_path = os.path.join(os.path.abspath(os.curdir), 'ckpt/train/dec')\n","# cp = tf.keras.callbacks.ModelCheckpoint(\n","#     checkpoint_path, monitor='batch_avg_loss', verbose=0, save_best_only=True,\n","#     save_weights_only=True, mode='min', save_freq=20\n","# )\n","cp = tf.keras.callbacks.ModelCheckpoint(\n","    dec_checkpoint_path, monitor='batch_avg_loss', verbose=0, save_best_only=True,\n","    save_weights_only=True, mode='min', save_freq=20\n",")\n","\n","es = tf.keras.callbacks.EarlyStopping(monitor='batch_avg_loss', patience=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vA2VzbTXx2CX","outputId":"58dc33ff-8d7b-40f7-b163-4a071a4d24ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 0.0009990576654672623.\n","Epoch 1/70\n","49/49 [==============================] - 109s 2s/step - batch_avg_loss: 0.6889 - accuracy: 22.3116 - lr: 9.9906e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 0.0009990433934187848.\n","Epoch 2/70\n","49/49 [==============================] - 109s 2s/step - batch_avg_loss: 0.6734 - accuracy: 22.1641 - lr: 9.9904e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 0.0009990148031025889.\n","Epoch 3/70\n","49/49 [==============================] - 108s 2s/step - batch_avg_loss: 0.6641 - accuracy: 21.7884 - lr: 9.9901e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 0.000998972011542621.\n","Epoch 4/70\n","49/49 [==============================] - 112s 2s/step - batch_avg_loss: 0.6537 - accuracy: 21.4299 - lr: 9.9897e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 0.0009989149029388013.\n","Epoch 5/70\n","49/49 [==============================] - 108s 2s/step - batch_avg_loss: 0.6368 - accuracy: 21.5883 - lr: 9.9891e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 0.0009988435943116803.\n","Epoch 6/70\n","49/49 [==============================] - 109s 2s/step - batch_avg_loss: 0.6263 - accuracy: 22.8488 - lr: 9.9884e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 0.0009987579698644343.\n","Epoch 7/70\n","49/49 [==============================] - 111s 2s/step - batch_avg_loss: 0.6079 - accuracy: 22.6823 - lr: 9.9876e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 0.0009986581466142183.\n","Epoch 8/70\n","49/49 [==============================] - 107s 2s/step - batch_avg_loss: 0.6023 - accuracy: 22.1019 - lr: 9.9866e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 0.0009985440087674645.\n","Epoch 9/70\n","49/49 [==============================] - 109s 2s/step - batch_avg_loss: 0.6413 - accuracy: 21.7090 - lr: 9.9854e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 0.0009984156733379326.\n","Epoch 10/70\n","29/49 [================>.............] - ETA: 44s - batch_avg_loss: 0.6091 - accuracy: 21.9838"]}],"source":["extractor.fit(dataset,epochs=epochs,\n","        callbacks=[LearningRateScheduler(lr_time_based_decay, verbose=1), cp,es],)"]},{"cell_type":"markdown","metadata":{"id":"KVuZJppXlWtV"},"source":["After training the model in different ways, we achieved a loss of 0.0122 and an accuracy of 22.05%. Though, the loss suggest pretty good probabilities for prediction, the accuracy was low. This behaviour by the model is difficult to understand or explain. Let's test on our test data to intuitively evaluate the models results.\n","\n","We also use the accuracy metrics here too. It checks the amount of characters were guessed right directly."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":354,"status":"ok","timestamp":1654109258942,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"CQNxT_h1M1VB","outputId":"a038dab5-8fd6-44f6-ef50-5218ef603d6b"},"outputs":[{"data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe2b7f9c050>"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["# with enc_vocab_size, loss is 0.0122, acc- 22.05%\n","# with dec_vocab_size, loss is \n","#model_path = os.path.join(os.path.abspath(os.curdir),'ckpt/best')\n","#extractor.save_weights(model_path)\n","extractor.load_weights(checkpoint_path)"]},{"cell_type":"markdown","metadata":{"id":"lxRHTJjUm4b7"},"source":["We must do somethings before we can move on:\n","\n","first, we define an extractor class that takes in the model and its weight and use it to extract the numbers. The class will serve as an API to the model itself.\n","\n","We also define a list of characters that we want the model to suppress when outputing results. We do this because the targets only contain numbers. Therefore, it will be unwise for the model to output letters and other characters in the first place.\n","\n","The model and all its methods are written below alongside comments explaining what it does."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CChhhxgf1ph0"},"outputs":[],"source":["# We create a list of characters that we do not want the model to output during inference.\n","# We do this because the targets are just numbers and contain no other characters.\n","enc_suppress_chars = ['<start>', '[UNK]', '<p>','#','<sp>','-','.',',', 'a', 'b',  'c', 'd', 'e', 'f',  'g', 'h',\n","                            'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u','v', 'w', 'x', 'y', 'z',]\n","#dec_suppress_chars = ['<start>', '[UNK]', '<p>','#','<sp>']\n","class StoreNumberExtractor(tf.Module):\n","  def __init__(self,embed, encoder, decoder, enc_id_from_char,char_from_id , suppress_chars , id_from_char=None, embed_dropout=None):\n","    # Initialize encoder, decoder,embedding layer and other text preprocessors. e.g enc_id_from_char:\n","    self.embed = embed\n","    if embed_dropout:\n","      self.embed_dropout = embed_dropout\n","      self.reg_emb = True\n","    else:\n","      self.reg_emb = False\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.encode_char = enc_id_from_char # Used to convert chars in any address to their respective ids.\n","    if id_from_char != None:\n","      self.id_from_char = id_from_char # If given we will use it to get the id of the characters we want the model to suppress.\n","      # The output should never generate padding,letters, unknown characters, or start tag.\n","      # Therefore, get the index of '<p>', '[UNK]', '<sp>', '#', '<start>' etc from the vocabulary:\n","      token_mask_ids = self.id_from_char(suppress_chars).numpy()\n","    else:\n","      token_mask_ids = self.encode_char(suppress_chars).numpy()\n","   \n","    self.decode_id = char_from_id # Used to convert the output of the model (which is in Index or id format) to respective characters.\n","\n","    # The output should never generate padding,letters, unknown characters, or start tag.\n","    # Therefore, get the index of '<p>', '[UNK]', '<sp>', '#', '<start>' etc from the vocabulary:\n","\n","    # Create a token mask by first creating an array of tokens (vocabulary) and then initialize all to False\n","    # Set the indexes of the '<p>', '[UNK]' and '<sp>' etc to True:\n","    token_mask = np.zeros([self.decode_id.vocabulary_size()], dtype= bool)\n","    token_mask[np.array(token_mask_ids)] = True\n","\n","    # set as a property of the class\n","    self.token_mask = token_mask\n","\n","    # Get the index of the '[START]' and '[END]' tokens:\n","    \n","    self.start_token = self.encode_char(['<start>']).numpy()[0]\n","    self.end_token = self.encode_char(['<end>']).numpy()[0]\n","\n","  #@tf.function\n","  def tokens_to_text(self, result_tokens):\n","    # This method converts index tokens to the string tokens by using the stringlookup class we initialized (self.encode_char) in the init() method:\n","    result_text_tokens = self.decode_id(result_tokens)\n","\n","    return result_text_tokens#.numpy()\n","\n","  #@tf.function\n","  def sample(self, logits, temperature=1.0):\n","\n","    # Add 2 new axis to the token_mask shape\n","    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n","\n","    # Set the logits for all masked tokens ('<p>','[START]', '[UNK]' etc) to -inf, so they are never chosen.\n","    # The logits (shape== (batch_size, vocab_size) here represents the independent probabilities of each word in the vocabulary.\n","    logits = tf.where(token_mask, -np.inf, logits)\n","    # If temperature is 0, get the max logit argument for each sample in the batch:\n","    # This argument represents the index of the word with the highest independent probability.\n","    if temperature == 0.0:\n","      new_tokens = tf.argmax(logits, axis=-1)\n","    # Else, sample from the independent probabilities of the logit:\n","    else: \n","      logits = tf.squeeze(logits, axis=1)\n","      new_tokens = tf.random.categorical(logits/temperature,\n","                                          num_samples=1)\n","    return new_tokens\n","\n","  #@tf.function\n","  def get_result(self, addresses, max_length=7,temperature=1.0):\n","    '''\n","       This is the main function that accesses the model. It takes the preprocessed addresses and passes it to the model and then processes result dynamically.\n","    '''\n","    \n","    # Encode the input\n","    #addresses = self.encode_char(addresses)\n","    \n","    batch_size = tf.shape(addresses)[0]\n","\n","    # Initialize the decoder\n","    new_tokens = tf.fill((batch_size,), self.start_token)\n","\n","    # Initialize decoder hidden state:\n","    dec_state = self.decoder.reset_state(batch_size)    \n","    \n","    # Embed and encode inputs:\n","    addresses = self.embed(addresses)\n","    encoded_addresses = self.encoder(addresses)\n","\n","    # Initialize the accumulators for the decoder output:\n","    result_tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)\n","    # Initialize a tf array that tracks the end_token of all samples in the batch:\n","    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n","    #print(f'outside loop: {new_tokens.shape}', dec_state.shape)\n","\n","    for t in tf.range(max_length):\n","      # Pass through decoder model:\n","      #print('yes')\n","      #print('Beginning of loop',new_tokens.shape)\n","      new_tokens = self.embed(new_tokens)\n","      new_tokens = tf.expand_dims(new_tokens, 1)\n","      #print('new_tokens', tf.shape(new_tokens))\n","      if self.reg_emb:\n","        new_tokens = self.embed_dropout(new_tokens)\n","\n","      dec_result, dec_state = self.decoder(new_tokens, dec_state, encoded_addresses)\n","      #print('NO')\n","      new_tokens = self.sample(dec_result, temperature)\n","      #print('After self.sample',new_tokens.shape)\n","      # If a sequence produces an `end_token`, set it `done`\n","      done = done | (new_tokens == self.end_token)\n","      # Once a sequence is done it only produces 0-padding.\n","      new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n","      #print('final new tokens', tf.shape(new_tokens))\n","\n","      # Collect the generated tokens\n","      result_tokens = result_tokens.write(t, new_tokens)\n","      if batch_size > 1:\n","        new_tokens = tf.squeeze(new_tokens)\n","      else:\n","        new_tokens = tf.squeeze(new_tokens, axis=[1])\n","\n","      if tf.reduce_all(done):\n","        break\n","\n","    # Convert the list of generates token ids to a list of strings.\n","    result_tokens = result_tokens.stack()\n","    result_tokens = tf.squeeze(result_tokens, -1)\n","    result_tokens = tf.transpose(result_tokens, [1, 0])\n","\n","    result_text = self.tokens_to_text(result_tokens)\n","\n","    return result_text\n","\n","  def process_text(self, addresses):\n","    # This method is the first to process text at inference.\n","    # It collects addresses as both string ( in the case of just one address) or as an iterable in the case of multiple addresses.\n","    # If in string format, put it into a list then pass unto the preprocess and pad methods described below.\n","    if isinstance(addresses, str):\n","      addresses = [addresses]\n","    addresses = self.preprocess(addresses)\n","    addresses = self.pad(addresses)\n","    return addresses\n","\n","  def pad(self, addresses, max_len=103):\n","    # This function processes a list of addresses and pads each address with '<p>' to an equal length of max_len(gotten from training dataset)\n","    for each_sample in addresses:\n","      if len(each_sample) < max_len:\n","        each_sample.extend(['<p>']*( max_len - len(each_sample)))\n","\n","    return addresses\n","\n","  def preprocess(self, addresses):\n","    # This method processes a list of address, converting each to a list of characters and appending the '<start>' and '<end>' tag at the start and eend of the list of characters.\n","    # It also converts the ' ' character to '<sp>'\n","    data = []\n","    for each_sample in addresses:\n","      sample_list = list(str(each_sample))# Convert to list of characters\n","      sample_list.insert(0, '<start>')#append the '<start>'tag\n","      sample_list.append('<end>') # append the '<end>' tag\n","      for ind, char in enumerate(sample_list): # Go through each character and convert any space character to a '<sp>'\n","        if char == ' ':\n","          sample_list[ind] = '<sp>'\n","      data.append(sample_list)\n","    return data #, index\n","\n","  def post_process(self, target):\n","    # This function processes the output of the model removing all unwanted characters and joining the predicted characters for each sample to form a full string of numbers.\n","    # Replace all occurences of '<end>', '<sp>', '#', '[UNK]', '<p>' with ''.\n","    target = tf.strings.regex_replace(target, b'<p>', b'')\n","    target = tf.strings.regex_replace(target, b'<end>', b'')\n","    target = tf.strings.regex_replace(target, b'<sp>', b'')\n","    target = tf.strings.regex_replace(target, b'#', b'')\n","    target = tf.strings.regex_replace(target, b'[UNK]', b'')\n","    # In the innermost axis, join all the elements of the list (in that axis) together to form one long string of numbers:\n","    target = tf.strings.reduce_join(target, axis = -1)\n","    return target\n","\n","  def extract(self,addresses,temperature=1.0):\n","    # This method is the interface to all other methods in this class.\n","    # Call this function with the addresses\n","\n","    # If the addresses are already processed and in tensorflow's tensor format, then just pass directly to the process_text method\n","    if isinstance(addresses, tf.Tensor):\n","      # If there is just one address in the tensor, add one more dimension to it\n","      if len(tf.shape(addresses)) == 1:\n","        addresses = tf.expand_dims(addresses, 0)\n","    else: # Else preprocess the raw addresses and turn them into a tensor using the process_text and encode_char methods. \n","      addresses = self.process_text(addresses)\n","      addresses = self.encode_char(addresses)\n","      if len(tf.shape(addresses)) == 1: #if tensor again contains only one address, add an extra axis to it.\n","        addresses = tf.expand_dims(addresses, 0)\n","    #print(tf.shape(addresses))\n","    output = self.get_result(addresses,temperature=temperature) # Pass the preprocessed addresses to the model for output generation using the get_result method.\n","    # Post process the output to a list of full strings\n","    output = self.post_process(output).numpy()\n","    # Iterate through each output and decode form byte string to a normal string.\n","    for ind, out in enumerate(output):\n","      output[ind] = out.decode()\n","\n","    return list(output) # Return result as a list.\n","\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKOz7JYF16c9"},"outputs":[],"source":["#extractor.load_weights(checkpoint_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xK_siPnMzFZH"},"outputs":[],"source":["ext  = StoreNumberExtractor(extractor.embed, extractor.encoder, extractor.decoder, ids_from_chars, chars_from_ids, suppress_chars = enc_suppress_chars)"]},{"cell_type":"markdown","metadata":{"id":"Vn51HPmtn1z2"},"source":["## Evaluation\n","\n","Let's define some functions to process our model's output and evaluate the accuracy of its output.\n","\n","We need this functions because the model was defined to produce results in strings, without padding and the start and end tags. So, we need to convert the results back to this format so we can evaluate the accuracy programmatically."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ti2v_jPYlB5"},"outputs":[],"source":["# So, this function pads the model's output to the max_length. It also trims the model's output to the max_length\n","def pad_result(y_pred,max_length=7):\n","  result = [] # define list to append processed outputs\n","  for pred in y_pred:\n","    if len(pred) < max_length: # if less than max length pad output\n","      pad_chars = ['<p>'] * (max_length - len(pred))\n","      pred += pad_chars\n","      result.append(pred)\n","\n","    elif len(pred) > max_length: # else trim output\n","      pred = pred[:max_length - 1]\n","      pred.append('<end>')\n","      result.append(pred)\n","      # y_pred.insert(index, temp)\n","    else:\n","      result.append(pred)\n","    #print(pred)\n","\n","  return result\n","\n","def get_accuracy(y_true, y_pred, batch_size=50):\n","  # Returns the average accuracy\n","  accuracy = Accuracy() # keras accuracy class\n","  # Calculate the accuracy in batches\n","  for start in range(0, len(y_true), batch_size):\n","    accuracy.update_state(y_true[start: start + batch_size], y_pred[start : start + batch_size]) #update accuracy metric\n","  acc = accuracy.result().numpy()\n","  return acc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":766020,"status":"ok","timestamp":1654113133090,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"dMAK7e4Ijdx3","outputId":"8b4a7220-4ed3-42b1-d7f1-f1a4d5ed0ad8"},"outputs":[{"name":"stdout","output_type":"stream","text":["['8823', '23154', '24951', '95798', '6393']\n"]},{"data":{"text/plain":["5000"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["result = []\n","batch = 100\n","for start in range(0,len(test_addresses), batch):\n","  result.extend(ext.extract(test_addresses[start: start + batch]))\n","\n","print(result[:5])\n","len(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1654113133091,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"dhFZQJGVuaPg","outputId":"9520a9f5-b675-48a0-d034-66ab01c8ee9d"},"outputs":[{"data":{"text/plain":["(<tf.Tensor: shape=(5, 7), dtype=int64, numpy=\n"," array([[43, 36, 36, 30, 31, 44,  0],\n","        [43, 30, 31, 29, 33, 32, 44],\n","        [43, 30, 32, 37, 33, 29, 44],\n","        [43, 37, 33, 35, 37, 36, 44],\n","        [43, 34, 31, 37, 31, 44,  0]])>, TensorShape([5000, 7]))"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["y_pred = process_column(result)\n","y_pred = pad_result(y_pred)\n","y_pred = ids_from_chars(y_pred)\n","y_pred[:5], y_pred.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1654113133093,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"KjMzQahWwNtt","outputId":"8a8d9755-7348-4566-cc89-683df2a0b680"},"outputs":[{"data":{"text/plain":["0.9357143"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["get_accuracy(test_targets[:100],y_pred, batch_size=50)"]},{"cell_type":"markdown","metadata":{"id":"Ka9bOfQ5q5JV"},"source":["## Conclusion\n","We see that the results are pretty good with an accuracy of 93.5%. However, accuracy does not account for the order of the characters. For example, a model output of 1234 instead of 7234 will have an accuracy of 75% but the model has failed to identify the first character which actually means a lot when it comes to the task of extracting store numbers from addresses.\n","\n","## Final Note/Suggestions\n","For store number extraction task, extracting all characters accurately and in the right order is paramount. Training with a BERT model might probably get better results. However, I believe that the task would be better modelled as an NER task but that would need labelling of the targets for example using the IBO tags or something similar. Nevertheless, this model has done really well already. Also, this model could be further trained close to 0 loss but I am not sure it will add to its performance significantly but it might be worth trying.\n","\n","Kindly note that we havent used this model to train on real world address samples. We have only trained on dummy samples. Therefore, the model intuitively might do even better in the real world especially if there are patterns in the position of the store numbers in the addresses. It might also do worse lol.\n","\n","Not withstanding, I believe that using this model plus regular expressions can be used to extract the store numbers accurately for close to 100% of the time. For example, we can use this model's output to compare with number chunks gotten using regular expressions in the raw address string and retrieve the one with the highest accuracy position-wise!\n","\n","Finally, I store all the results of the test dataset in a csv file for more intuitive understanding of the model's performance."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":417,"status":"ok","timestamp":1654113769611,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"a_DRbLuNy8N6","outputId":"9cc4e89d-8701-4a21-d037-1b5a59433795"},"outputs":[{"data":{"text/plain":["['8823',\n"," '23154',\n"," '24951',\n"," '95798',\n"," '6393',\n"," '97028',\n"," '81507',\n"," '94703',\n"," '81417',\n"," '25780']"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["test_output = []\n","for start in range(0,len(test_targets), batch):\n","  temp = ext.tokens_to_text(test_targets[start: start + batch])\n","  temp = tf.strings.regex_replace(temp, b'<start>', b'')\n","  temp = ext.post_process(temp)\n","  test_output.extend(temp.numpy())\n","\n","for ind, each in enumerate(test_output):\n","  test_output[ind] = each.decode()\n","\n","test_output[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EsVHJsR8wc7k"},"outputs":[],"source":["csv = pd.DataFrame({'Real targets': test_output , 'pred targets': result})\n","csv.to_csv(file_path + '/result.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":501,"status":"ok","timestamp":1654114078048,"user":{"displayName":"jeffrey otoibhi","userId":"11067368294353522262"},"user_tz":-60},"id":"0jSha56fvd7V","outputId":"31234a20-5f63-4f34-e392-0968520a11df"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-122e1f05-aacf-4523-abc6-52d290819d72\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Real targets</th>\n","      <th>pred targets</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>8823</td>\n","      <td>8823</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>23154</td>\n","      <td>23154</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>24951</td>\n","      <td>24951</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>95798</td>\n","      <td>95798</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6393</td>\n","      <td>6393</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>97028</td>\n","      <td>97028</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>81507</td>\n","      <td>81507</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>94703</td>\n","      <td>94703</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>81417</td>\n","      <td>81417</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>25780</td>\n","      <td>25782</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>34636</td>\n","      <td>34636</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>3908</td>\n","      <td>3908</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>16164</td>\n","      <td>16164</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>11126</td>\n","      <td>11126</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>63398</td>\n","      <td>63398</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>81031</td>\n","      <td>81031</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>6316</td>\n","      <td>63161</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>76328</td>\n","      <td>76328</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>47924</td>\n","      <td>47924</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>81229</td>\n","      <td>81229</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>43368</td>\n","      <td>43368</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>41134</td>\n","      <td>41134</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>79778</td>\n","      <td>79778</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>57497</td>\n","      <td>57497</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>85130</td>\n","      <td>85130</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>29454</td>\n","      <td>18604</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>50929</td>\n","      <td>50929</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>78630</td>\n","      <td>6271</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>79708</td>\n","      <td>79708</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>1241</td>\n","      <td>1241</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>22688</td>\n","      <td>22688</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>84557</td>\n","      <td>84557</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>46190</td>\n","      <td>46090</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>29784</td>\n","      <td>29784</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>64082</td>\n","      <td>64575</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>64009</td>\n","      <td>64009</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>20895</td>\n","      <td>20895</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>61261</td>\n","      <td>61261</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>84447</td>\n","      <td>84454</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>58586</td>\n","      <td>58586</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>82890</td>\n","      <td>82890</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>76353</td>\n","      <td>76353</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>23820</td>\n","      <td>23820</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>76380</td>\n","      <td>76380</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>62169</td>\n","      <td>62169</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>64897</td>\n","      <td>64897</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>72604</td>\n","      <td>43205</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>92046</td>\n","      <td>92046</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>88336</td>\n","      <td>88336</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>21582</td>\n","      <td>21582</td>\n","    </tr>\n","    <tr>\n","      <th>50</th>\n","      <td>90234</td>\n","      <td>90234</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>78716</td>\n","      <td>78716</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>41494</td>\n","      <td>41494</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>46456</td>\n","      <td>46456</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>54976</td>\n","      <td>54976</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>47640</td>\n","      <td>47640</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>88356</td>\n","      <td>88356</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>18293</td>\n","      <td>18293</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>13095</td>\n","      <td>13095</td>\n","    </tr>\n","    <tr>\n","      <th>59</th>\n","      <td>85595</td>\n","      <td>85595</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-122e1f05-aacf-4523-abc6-52d290819d72')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-122e1f05-aacf-4523-abc6-52d290819d72 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-122e1f05-aacf-4523-abc6-52d290819d72');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   Real targets pred targets\n","0          8823         8823\n","1         23154        23154\n","2         24951        24951\n","3         95798        95798\n","4          6393         6393\n","5         97028        97028\n","6         81507        81507\n","7         94703        94703\n","8         81417        81417\n","9         25780        25782\n","10        34636        34636\n","11         3908         3908\n","12        16164        16164\n","13        11126        11126\n","14        63398        63398\n","15        81031        81031\n","16         6316        63161\n","17        76328        76328\n","18        47924        47924\n","19        81229        81229\n","20        43368        43368\n","21        41134        41134\n","22        79778        79778\n","23        57497        57497\n","24        85130        85130\n","25        29454        18604\n","26        50929        50929\n","27        78630         6271\n","28        79708        79708\n","29         1241         1241\n","30        22688        22688\n","31        84557        84557\n","32        46190        46090\n","33        29784        29784\n","34        64082        64575\n","35        64009        64009\n","36        20895        20895\n","37        61261        61261\n","38        84447        84454\n","39        58586        58586\n","40        82890        82890\n","41        76353        76353\n","42        23820        23820\n","43        76380        76380\n","44        62169        62169\n","45        64897        64897\n","46        72604        43205\n","47        92046        92046\n","48        88336        88336\n","49        21582        21582\n","50        90234        90234\n","51        78716        78716\n","52        41494        41494\n","53        46456        46456\n","54        54976        54976\n","55        47640        47640\n","56        88356        88356\n","57        18293        18293\n","58        13095        13095\n","59        85595        85595"]},"execution_count":88,"metadata":{},"output_type":"execute_result"}],"source":["csv.head(60)"]},{"cell_type":"markdown","metadata":{"id":"GrVnrnxH3kSu"},"source":["Comparing the results practically, we see that the model did amazingly well for a lot of the samples. However, take a look at sample no 34 and 46 for example. it got only the first 2 digits in the sample 36 and totally misses sample 46.\n","\n","You can look at more samples for comparisons."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xn6cc7Xf4RSI"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Model_train.pynb","provenance":[],"mount_file_id":"1AIT35EUFZFbDpiXMVl8UDyywoBbLgC0a","authorship_tag":"ABX9TyPctjs4qjUEHkNdyFM0cMB8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}